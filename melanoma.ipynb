{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\pythonProjects\\MelanomaClassification\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "b:\\pythonProjects\\MelanomaClassification\\.venv\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom DataLoader (Assuming you have this defined)\n",
    "from DataLoader import Data_Loader, ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data set\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"cdeotte/jpeg-melanoma-512x512\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
       "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
       "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
       "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
       "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
       "\n",
       "  diagnosis benign_malignant  target  \n",
       "0   unknown           benign       0  \n",
       "1   unknown           benign       0  \n",
       "2     nevus           benign       0  \n",
       "3   unknown           benign       0  \n",
       "4   unknown           benign       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')  \n",
    "train_df = train_df.drop(['tfrecord','width','height'],axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33126, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to fold\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "df = train_df.sample(frac=1).reset_index(drop=True) #shuffles the entire dataset randomly.\n",
    "df['kfold'] = -1 #adds a new column 'kfold' to the dataframe and initializes all values to -1.\n",
    "y = train_df.target.values\n",
    "kf = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=32) #creates a StratifiedKFold object with 5 splits.\n",
    "\n",
    "for fold,(x,y) in enumerate(kf.split(X=df,y=y)):\n",
    "    df.loc[y,'kfold'] = fold\n",
    "df.to_csv('train_fold_tpu.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0296109</td>\n",
       "      <td>IP_3970734</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_2589797</td>\n",
       "      <td>IP_9122609</td>\n",
       "      <td>male</td>\n",
       "      <td>65.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_3448409</td>\n",
       "      <td>IP_5867829</td>\n",
       "      <td>male</td>\n",
       "      <td>50.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_1493314</td>\n",
       "      <td>IP_2394927</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_7649562</td>\n",
       "      <td>IP_4295309</td>\n",
       "      <td>male</td>\n",
       "      <td>60.0</td>\n",
       "      <td>palms/soles</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id   sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_0296109  IP_3970734  male        40.0                         torso   \n",
       "1  ISIC_2589797  IP_9122609  male        65.0                         torso   \n",
       "2  ISIC_3448409  IP_5867829  male        50.0               upper extremity   \n",
       "3  ISIC_1493314  IP_2394927  male        45.0               lower extremity   \n",
       "4  ISIC_7649562  IP_4295309  male        60.0                   palms/soles   \n",
       "\n",
       "  diagnosis benign_malignant  target  kfold  \n",
       "0   unknown           benign       0      1  \n",
       "1   unknown           benign       0      3  \n",
       "2   unknown           benign       0      4  \n",
       "3     nevus           benign       0      1  \n",
       "4   unknown           benign       0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_fold_tpu.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet \n",
    "\n",
    "class EfficientNetClassifier(nn.Module):\n",
    "    def __init__(self, model_version='b4', num_classes=1, dropout_rate=0.3):\n",
    "        super(EfficientNetClassifier, self).__init__()\n",
    "        \n",
    "        model_name = f'efficientnet-{model_version}'\n",
    "        # Instance of the EfficientNet model with pre-trained weights\n",
    "        self.feature_extractor = EfficientNet.from_pretrained(model_name)\n",
    "        \n",
    "        # Get the number of input features from the last layer of EfficientNet\n",
    "        num_features = self.feature_extractor._fc.in_features\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),  # Increased dropout\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Increased dropout\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Pass images through the pre-trained EfficientNet feature extractor\n",
    "        features = self.feature_extractor.extract_features(images)\n",
    "        \n",
    "        # Pool and reshape features\n",
    "        # Reduce the spatial dimensions of the features (e.g., [batch_size, channels, height, width] -> [batch_size, channels])\n",
    "        pooled_features = nn.functional.adaptive_avg_pool2d(features, 1).reshape(batch_size, -1)\n",
    "        \n",
    "        #  Pass the pooled features through the sequential classification layers\n",
    "        output = self.classifier(pooled_features)\n",
    "        \n",
    "        return output  # Return logits directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "class MobileNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=1, dropout_rate=0.3):\n",
    "        super(MobileNetV2Classifier, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained MobileNetV2 model\n",
    "        self.feature_extractor = mobilenet_v2(weights='MobileNet_V2_Weights.DEFAULT')\n",
    "        \n",
    "        # Get the number of input features from the last layer of MobileNetV2\n",
    "        num_features = self.feature_extractor.last_channel\n",
    "        \n",
    "        # Remove the classifier part of the pre-trained model\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])\n",
    "        \n",
    "        # Define a custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Extract features using the MobileNetV2 model\n",
    "        features = self.feature_extractor(images)\n",
    "        features = features.mean([2, 3])  # Global Average Pooling\n",
    "        \n",
    "        # Pass pooled features through the classifier\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output  # Return logits directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "modelEffNet = EfficientNetClassifier(model_version='b0', num_classes=1, dropout_rate=0.5)\n",
    "modelMobileNet = MobileNetV2Classifier()\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"min\", delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif (self.mode == \"min\" and score > self.best_score - self.delta) or \\\n",
    "             (self.mode == \"max\" and score < self.best_score + self.delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "# Configuration Parameters\n",
    "CONFIG = {\n",
    "    'normalization_mean': (0.485, 0.456, 0.406),\n",
    "    'normalization_std': (0.229, 0.224, 0.225),\n",
    "    'image_size': (384, 384),\n",
    "    'batch_size': 28,\n",
    "    'num_epochs': 3,\n",
    "    'learning_rate': 3e-5,\n",
    "    'weight_decay': 1e-4,\n",
    "    'early_stopping_patience': 5,\n",
    "}\n",
    "\n",
    "# Define Focal Loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.5, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epoch function with AMP and debug for batch size\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(tqdm(dataloader)):\n",
    "        # Print batch size for debugging\n",
    "        # print(f\"Training Batch {batch_idx} - Batch size: {data.shape[0]}\")\n",
    "        \n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use AMP for mixed precision training\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        \n",
    "        # Backward pass with AMP scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * data.shape[0]\n",
    "        prob_outputs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        predictions.extend(prob_outputs.flatten())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "    return total_loss / len(dataloader.dataset), predictions, targets\n",
    "\n",
    "# Validation epoch function with AMP and debug for batch size\n",
    "def validate_epoch(model, dataloader, criterion, device, scaler):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(tqdm(dataloader)):\n",
    "            # Print batch size for debugging\n",
    "            # print(f\"Validation Batch {batch_idx} - Batch size: {data.shape[0]}\")\n",
    "            \n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Use AMP for mixed precision inference\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            \n",
    "            total_loss += loss.item() * data.shape[0]\n",
    "            prob_outputs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            predictions.extend(prob_outputs.flatten())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(dataloader.dataset), predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=CONFIG['normalization_mean'], std=CONFIG['normalization_std'], max_pixel_value=255.0),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.7),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75),\n",
    "        A.CLAHE(clip_limit=4.0, p=0.7),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=CONFIG['normalization_mean'], std=CONFIG['normalization_std'], max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def train_model(model, train_fold, df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Load data transformations\n",
    "    train_transforms = get_train_transforms()\n",
    "    valid_transforms = get_valid_transforms()\n",
    "    \n",
    "    # Prepare training and validation datasets\n",
    "    image_directory = 'data/train'\n",
    "    train_df = df[df.kfold != train_fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == train_fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = Data_Loader(image_directory, train_df.image_name.tolist(), train_df.target.values, train_transforms).dataset\n",
    "    valid_dataset = Data_Loader(image_directory, valid_df.image_name.tolist(), valid_df.target.values, valid_transforms).dataset\n",
    "\n",
    "    # WeightedRandomSampler for handling class imbalance\n",
    "    class_sample_counts = train_df['target'].value_counts().sort_index().values\n",
    "    class_weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "    sample_weights = class_weights[train_df['target'].values]\n",
    "    weighted_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # DataLoader with WeightedRandomSampler for training\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, num_workers=4, pin_memory=True, batch_size=CONFIG['batch_size'],sampler=weighted_sampler, drop_last=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, num_workers=4, pin_memory=True, batch_size=CONFIG['batch_size'], shuffle=False,drop_last=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    loss_criterion = FocalLoss(alpha=0.5, gamma=2.0).to(device)\n",
    "\n",
    "    # Use AdamW optimizer with weight decay for better generalization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "    # Scheduler: CosineAnnealingWarmRestarts or ReduceLROnPlateau\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    # Initialize AMP scaler\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    best_valid_pr_auc = 0.0\n",
    "    best_model = None\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_predictions, train_targets = train_epoch(model, train_loader, loss_criterion, optimizer, device, scaler)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss, valid_predictions, valid_targets = validate_epoch(model, valid_loader, loss_criterion, device, scaler)\n",
    "        \n",
    "        # Calculate PR AUC scores\n",
    "        train_pr_auc = average_precision_score(train_targets, train_predictions)\n",
    "        valid_pr_auc = average_precision_score(valid_targets, valid_predictions)\n",
    "        train_f1 = f1_score(train_targets, np.round(train_predictions), pos_label=1, zero_division=0)\n",
    "        valid_f1 = f1_score(valid_targets, np.round(valid_predictions), pos_label=1, zero_division=0)\n",
    "        print(f\"Train F1-Score: {train_f1:.6f}\")\n",
    "        print(f\"Valid F1-Score: {valid_f1:.6f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}: Train Loss: {train_loss:.4f}, Train PR AUC: {train_pr_auc:.6f}, Valid Loss: {valid_loss:.4f}, Valid PR AUC: {valid_pr_auc:.6f}\")\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()  # If using CosineAnnealingWarmRestarts\n",
    "        # scheduler.step(valid_pr_auc)  # If using ReduceLROnPlateau, use validation PR AUC to trigger adjustments\n",
    "        \n",
    "        # Save the best model based on validation PR AUC\n",
    "        if valid_pr_auc > best_valid_pr_auc:\n",
    "            best_valid_pr_auc = valid_pr_auc\n",
    "            best_model = model.state_dict().copy()\n",
    "            no_improve_epochs = 0\n",
    "            print(\"New best model saved based on validation PR AUC\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if no_improve_epochs >= CONFIG['early_stopping_patience']:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs with no improvement in PR AUC\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    return model, best_model, valid_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:50<00:00,  8.59it/s]\n",
      "100%|██████████| 236/236 [00:19<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.976983\n",
      "Valid F1-Score: 0.995671\n",
      "Epoch 1/3: Train Loss: 0.0087, Train PR AUC: 0.998180, Valid Loss: 0.0002, Valid PR AUC: 1.000000\n",
      "New best model saved based on validation PR AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.90it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.997635\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 2/3: Train Loss: 0.0015, Train PR AUC: 0.999823, Valid Loss: 0.0001, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:47<00:00,  8.83it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999014\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 3/3: Train Loss: 0.0008, Train PR AUC: 0.999984, Valid Loss: 0.0001, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.88it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999661\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 1/3: Train Loss: 0.0003, Train PR AUC: 0.999998, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n",
      "New best model saved based on validation PR AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:47<00:00,  8.77it/s]\n",
      "100%|██████████| 236/236 [00:19<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999584\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 2/3: Train Loss: 0.0002, Train PR AUC: 0.999999, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.91it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999733\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 3/3: Train Loss: 0.0001, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:45<00:00,  8.93it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999811\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 1/3: Train Loss: 0.0001, Train PR AUC: 0.999999, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n",
      "New best model saved based on validation PR AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.92it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 1.000000\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 2/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.91it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999847\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 3/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:45<00:00,  8.93it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999963\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 1/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n",
      "New best model saved based on validation PR AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:45<00:00,  8.94it/s]\n",
      "100%|██████████| 236/236 [00:19<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999849\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 2/3: Train Loss: 0.0001, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.91it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999887\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 3/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:45<00:00,  8.94it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.999925\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 1/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n",
      "New best model saved based on validation PR AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:46<00:00,  8.91it/s]\n",
      "100%|██████████| 236/236 [00:19<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 1.000000\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 2/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [01:45<00:00,  8.93it/s]\n",
      "100%|██████████| 236/236 [00:18<00:00, 12.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 1.000000\n",
      "Valid F1-Score: 1.000000\n",
      "Epoch 3/3: Train Loss: 0.0000, Train PR AUC: 1.000000, Valid Loss: 0.0000, Valid PR AUC: 1.000000\n",
      "Average F1 score across different folds: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_arr = []\n",
    "model_type = modelMobileNet \n",
    "df = pd.read_csv('train_fold_tpu.csv')\n",
    "for i in range(5):\n",
    "    cur_model, best_model, model_f1 = train_model(model_type, i, df=df)\n",
    "    f1_arr.append(model_f1)\n",
    "    \n",
    "    # Save the model using a valid file name\n",
    "    torch.save(best_model, f'model/fold_{i}_MobileNet.pth')\n",
    "\n",
    "average_f1_score = np.mean(f1_arr)\n",
    "print(f\"Average F1 score across different folds: {average_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is testing with ISSIC 2019 images to verify the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping images: 0\n",
      "False Positives: 170\n",
      "False Negatives: 0\n",
      "Confusion Matrix:\n",
      "[[5719  170]\n",
      " [   0 4095]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall AUC (PR AUC): 0.9999\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "model = modelMobileNet\n",
    "# Load model weights\n",
    "model_path = \"model/fold_4_MobileNet.pth\"\n",
    "model.load_state_dict(torch.load(model_path,weights_only=True))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Load the train dataset (ISIC 2020) and test dataset (ISIC 2019)\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test_ISIC_2019/ISIC_2019_Training_GroundTruth.csv')\n",
    "\n",
    "# Rename and drop columns in the 2019 dataset\n",
    "df_test.rename(columns={\"MEL\": 'target'}, inplace=True)\n",
    "df_test.drop(columns=['NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'], inplace=True)\n",
    "\n",
    "#remove overlapping images\n",
    "common_images = set(df_train['image_name']).intersection(set(df_test['image']))\n",
    "print(f\"Number of overlapping images: {len(common_images)}\")\n",
    "# filter out the common images name\n",
    "df_test_filtered = df_test[\n",
    "    (~df_test['image'].isin(common_images)) & \n",
    "    (~df_test['image'].str.endswith('downsampled'))\n",
    "]\n",
    "\n",
    "\n",
    "# Determine the number of samples to take\n",
    "total_samples = 10000\n",
    "positive_samples = 4100  # Number of samples with target = 1\n",
    "negative_samples = total_samples - positive_samples\n",
    "\n",
    "# Sample positive and negative cases\n",
    "df_positive = df_test_filtered[df_test_filtered['target'] == 1].sample(n=positive_samples, random_state=23)\n",
    "df_negative = df_test_filtered[df_test_filtered['target'] == 0].sample(n=negative_samples, random_state=3)\n",
    "\n",
    "# Combine the sampled DataFrames\n",
    "df_sampled = pd.concat([df_positive, df_negative]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare image and target lists\n",
    "test_im = df_sampled.image.values.tolist()\n",
    "test_target = df_sampled.target.values.tolist()\n",
    "\n",
    "# Define validation transformations\n",
    "valid_transpose = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Loading validation dataset\n",
    "test_dataset = Data_Loader(\n",
    "    'data/test_ISIC_2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input', \n",
    "    test_im, \n",
    "    test_target, \n",
    "    valid_transpose\n",
    ").get(batch_size=32, shuffle=False, num_workers=6)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_dataset:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Convert predictions to binary\n",
    "binary_predictions = np.round(all_predictions).flatten()\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Identify incorrect predictions\n",
    "false_positives = np.where((binary_predictions == 1) & (all_targets == 0))[0]\n",
    "false_negatives = np.where((binary_predictions == 0) & (all_targets == 1))[0]\n",
    "\n",
    "print(f\"False Positives: {len(false_positives)}\")\n",
    "print(f\"False Negatives: {len(false_negatives)}\")\n",
    "\n",
    "# Visualize the incorrect predictions with image name\n",
    "def visualize_incorrect(indices, title):\n",
    "    for idx in indices:\n",
    "        img_name = test_im[idx]\n",
    "        img_id = df_sampled.loc[idx, \"image\"]  # Assuming \"image\" column corresponds to ID\n",
    "        img_path = f'data/test_ISIC_2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input/{img_name}.jpg'\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{title}: True={test_target[idx]}, Predicted={binary_predictions[idx]}\\nImage Name: {img_name}, ID: {img_id}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# # Visualize false positives and false negatives\n",
    "# print(\"Visualizing False Positives:\")\n",
    "# visualize_incorrect(false_positives, \"False Positive\")\n",
    "\n",
    "# print(\"Visualizing False Negatives:\")\n",
    "# visualize_incorrect(false_negatives, \"False Negative\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(all_targets, binary_predictions)\n",
    "\n",
    "# Print the confusion matrix values\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display the confusion matrix using matplotlib\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-Melanoma\", \"Melanoma\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "pr_auc = average_precision_score(all_targets, all_predictions)\n",
    "print(f\"Precision-Recall AUC (PR AUC): {pr_auc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
